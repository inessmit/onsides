{
  "defaults": {
    "construct_training_data": {
      "method": [0],
      "nwords": [125],
      "section": ["AR"]
    },
    "fit_clinicalbert": {
      "max-length": [-1],
      "batch-size": [-1],
      "epochs": [25],
      "learning-rate": [1e-6],
      "ifexists": ["quit"],
      "network": ["Bio_ClinicalBERT"]
    },
    "analyze_results": {
      "skip-train": [true],
      "network": ["Bio_ClinicalBERT"]
    },
    "compile_results": {
      "group-function": ["mean"]
    }
  },

  "experiments":{
    "0": {
      "name": "Compare Example Construction Methods",
      "description": "We have five different methods of creating the example strings based on whether or not we replace the adverse event term with a common term 'EVENT', prepend the adverse event term to the beginning of the example string, or replace the event with a nonsense (unmappable) term. These different methods are built using the construct_training_data.py script using the method argument to choose between the five (0, 1, 2, 3, or 4). See construct_training_data.py for more detials.",
      "factor": {
        "script": "construct_training_data",
        "parameter": "method",
        "labels": [
          "Substitute Term, Prepend Term",
          "Prepend Term Only",
          "Substitute Term Only",
          "No Changes",
          "Subsitute with Nonsense",
          "Random Sentences"
        ]
      },
      "construct_training_data": {
        "method": [0, 1, 2, 3, 4, 5],
        "nwords": [30],
        "section": ["AR"]
      },
      "fit_clinicalbert": {
        "max-length": [32]
      }
    },

    "1": {
      "name": "Compare PubMedBERT to ClinicalBERT",
      "description": "PubMedBERT is a new neural network made available from microsoft. We'd like to know how it compares to using ClinicalBERT for our task.",
      "factor": {
        "script": "fit_clinicalbert",
        "parameter": "network",
        "labels": [
          "ClinicalBERT",
          "PubMedBERT"
        ]
      },
      "construct_training_data": {
        "nwords": [60]
      },
      "fit_clinicalbert": {
        "network": ["Bio_ClinicalBERT", "microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract"],
        "epochs": [20]
      }
    },

    "2": {
      "name": "Compare values for nwords",
      "description": "The number of words we extract from the labels and use to make the examples is a core component of the reference set. Essentially the thinking is the more words the better, however, that also exponentially increases the running time. In this experiment we adjust the number of words used in the example strings from 30 up to 250 (our maximum).",
      "factor": {
        "script": "construct_training_data",
        "parameter": "nwords",
        "labels": [
          "LLT Term Only", "6", "15", "30", "60", "125", "250"
        ]
      },
      "construct_training_data": {
        "nwords": [3, 6, 15, 30, 60, 125, 250]
      }
    },

    "3": {
      "name": "Compare grouping functions",
      "description": "Each drug, event pair has multiple example strings and each of these example strings will have its own prediction. In order to reduce this down to each drug, event pair we have to decide how to group the data together.",
      "factor": {
        "script": "compile_results",
        "parameter": "group-function",
        "labels": ["mean", "max", "median", "min"]
      },
      "compile_results": {
        "group-function": ["mean", "max", "median", "min"]
      }
    },

    "4": {
      "name": "Compare performance across AR, BW, and WP sections",
      "description": "Compare performance for each of the sections: Adverse Reactions (AR), Boxed Warnings (BW), and Warnings and Precautions (WP). For this experiment we train all of them independently from the pre-trained ClinicalBERT network. Also compare grouping all of the sections together. We may see improved performance of the lower frequency sections in this scenario.",
      "factor": {
        "script": "construct_training_data",
        "parameter": "section",
        "labels": ["AR", "BW", "WP", "ALL"]
      },
      "construct_training_data": {
        "section": ["AR", "BW", "WP", "ALL"]
      }
    },

    "5": {
      "name": "Investigate pretraining with AR for Boxed Warning performance",
      "description": "Boxed warnings do not have as many training examples and suffer in performance. We want to investigate using the pretrained model from the Adverse Reactions sections with no additional training and with additional training to see how it performs.",
      "factor": {
        "script": "fit_clinicalbert",
        "parameter": ["network", "epochs"],
        "labels": [
          "PreTrain:AR, Fit 0 Epochs",
          "PreTrain:AR, Fit 10 Epochs",
          "PreTrain:BW, Fit 0 Epochs",
          "PreTrain:BW, Fit 10 Epochs"
        ]
      },
      "construct_training_data": {
        "section": ["BW"]
      },
      "fit_clinicalbert": {
        "network": [
          "bestepoch-bydrug-CB_0-AR-125_222_24_25_1e-06_256_32.pth",
          "bestepoch-bydrug-CB_0-BW-125_222_24_25_1e-06_256_32.pth"
        ],
        "epochs": [0, 10]
      }
    },

    "6": {
      "name": "Compare string before to string after",
      "description": "When constructing the training data we have the choice to take words preceding the event term, following the event term, or an event split of both (default). This experiment compares the performance of those three options. This could be part of experiment 0, but that seems to be getting crowded.",
      "factor": {
        "script": "construct_training_data",
        "parameter": "method",
        "labels": [
          "After Only",
          "87.5% After",
          "75% After",
          "Before and After",
          "75% Before",
          "87.5% Before",
          "Before Only"
        ]
      },
      "construct_training_data": {
        "method": [7, 8, 9, 0, 10, 11, 6],
        "nwords": [30],
        "section": ["AR"]
      },
      "fit_clinicalbert": {
        "max-length": [32]
      }
    },

    "7": {
      "name": "Compare values for nwords",
      "description": "Same as experiment two except for reference method 8, rather than 0. The number of words we extract from the labels and use to make the examples is a core component of the reference set. Essentially the thinking is the more words the better, however, that also exponentially increases the running time. In this experiment we adjust the number of words used in the example strings from 30 up to 250 (our maximum).",
      "factor": {
        "script": "construct_training_data",
        "parameter": "nwords",
        "labels": [
          "LLT Term Only", "6", "15", "30", "60", "125", "250"
        ]
      },
      "construct_training_data": {
        "method": [8],
        "nwords": [3, 6, 15, 30, 60, 125, 250]
      },
      "fit_clinicalbert": {
        "epochs": [10]
      }
    }

  }
}

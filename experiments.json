{
  "defaults": {
    "construct_training_data": {
      "method": [0],
      "nwords": [125],
      "section": ["AR"]
    },
    "fit_clinicalbert": {
      "max-length": [-1],
      "batch-size": [-1],
      "epochs": [25],
      "learning-rate": [1e-6],
      "ifexists": ["quit"],
      "network": ["Bio_ClinicalBERT"],
      "refsource": ["all"],
      "split-method": ["24"]
    },
    "analyze_results": {
      "skip-train": [true],
      "network": ["Bio_ClinicalBERT"]
    },
    "compile_results": {
      "group-function": ["mean"]
    }
  },

  "experiments":{
    "0": {
      "name": "Compare Example Construction Methods, ClinicalBERT",
      "description": "We have six different methods of creating the example strings based on whether or not we replace the adverse event term with a common term 'EVENT', prepend the adverse event term to the beginning of the example string, or replace the event with a nonsense (unmappable) term. These different methods are built using the construct_training_data.py script using the method argument to choose between the five (0, 1, 2, 3, 4, 5, or 12). See construct_training_data.py for more detials.",
      "factor": {
        "script": "construct_training_data",
        "parameter": "method",
        "labels": [
          "Substitute Term, Prepend Term",
          "Prepend Term Only",
          "Substitute Term Only",
          "No Changes",
          "Subsitute with Nonsense",
          "Random Sentences",
          "Substitute Term, Prepend Term, Prepend Source"
        ]
      },
      "construct_training_data": {
        "method": [0, 1, 2, 3, 4, 5, 12],
        "nwords": [30],
        "section": ["AR"]
      },
      "fit_clinicalbert": {
        "max-length": [32]
      }
    },

    "0A": {
      "name": "Compare Example Construction Methods, PubMedBERT",
      "description": "We have six different methods of creating the example strings based on whether or not we replace the adverse event term with a common term 'EVENT', prepend the adverse event term to the beginning of the example string, or replace the event with a nonsense (unmappable) term. These different methods are built using the construct_training_data.py script using the method argument to choose between the five (0, 1, 2, 3, 4, 5, or 12). See construct_training_data.py for more detials.",
      "factor": {
        "script": "construct_training_data",
        "parameter": "method",
        "labels": [
          "Substitute Term, Prepend Term",
          "Prepend Term Only",
          "Substitute Term Only",
          "No Changes",
          "Subsitute with Nonsense",
          "Random Sentences",
          "Substitute Term, Prepend Term, Prepend Source"
        ]
      },
      "construct_training_data": {
        "method": [0, 1, 2, 3, 4, 5, 12],
        "nwords": [30],
        "section": ["AR"]
      },
      "fit_clinicalbert": {
        "network": ["microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract"],
        "max-length": [32]
      }
    },

    "1": {
      "name": "Compare PubMedBERT to ClinicalBERT",
      "description": "PubMedBERT is a new neural network made available from microsoft. We'd like to know how it compares to using ClinicalBERT for our task.",
      "factor": {
        "script": "fit_clinicalbert",
        "parameter": "network",
        "labels": [
          "ClinicalBERT",
          "PubMedBERT"
        ]
      },
      "construct_training_data": {
        "nwords": [60]
      },
      "fit_clinicalbert": {
        "network": ["Bio_ClinicalBERT", "microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract"],
        "epochs": [20]
      }
    },

    "2": {
      "name": "Compare values for nwords",
      "description": "The number of words we extract from the labels and use to make the examples is a core component of the reference set. Essentially the thinking is the more words the better, however, that also exponentially increases the running time. In this experiment we adjust the number of words used in the example strings from 30 up to 250 (our maximum).",
      "factor": {
        "script": "construct_training_data",
        "parameter": "nwords",
        "labels": [
          "LLT Term Only", "6", "15", "30", "60", "125", "250"
        ]
      },
      "construct_training_data": {
        "nwords": [3, 6, 15, 30, 60, 125, 250]
      }
    },

    "3": {
      "name": "Compare grouping functions",
      "description": "Each drug, event pair has multiple example strings and each of these example strings will have its own prediction. In order to reduce this down to each drug, event pair we have to decide how to group the data together.",
      "factor": {
        "script": "compile_results",
        "parameter": "group-function",
        "labels": ["mean", "max", "median", "min"]
      },
      "compile_results": {
        "group-function": ["mean", "max", "median", "min"]
      }
    },

    "4": {
      "name": "Compare performance across AR, BW, and WP sections",
      "description": "Compare performance for each of the sections: Adverse Reactions (AR), Boxed Warnings (BW), and Warnings and Precautions (WP). For this experiment we train all of them independently from the pre-trained ClinicalBERT network. Also compare grouping all of the sections together. We may see improved performance of the lower frequency sections in this scenario.",
      "factor": {
        "script": "construct_training_data",
        "parameter": "section",
        "labels": ["AR", "BW", "WP", "ALL", "ARBW"]
      },
      "construct_training_data": {
        "section": ["AR", "BW", "WP", "ALL", "ARBW"]
      }
    },

    "4A": {
      "name": "Compare performance across AR, BW, and WP sections, method 14 (prepend refsource, 87.5 After)",
      "description": "Compare performance for each of the sections: Adverse Reactions (AR), Boxed Warnings (BW), and Warnings and Precautions (WP). For this experiment we train all of them independently from the pre-trained ClinicalBERT network. Also compare grouping all of the sections together. We may see improved performance of the lower frequency sections in this scenario.",
      "factor": {
        "script": "construct_training_data",
        "parameter": "section",
        "labels": ["AR", "BW", "WP", "ALL", "ARBW"]
      },
      "construct_training_data": {
        "section": ["AR", "BW", "WP", "ALL", "ARBW"],
        "method": [14]
      }
    },

    "5": {
      "name": "Investigate pretraining with AR for Boxed Warning performance",
      "description": "Boxed warnings do not have as many training examples and suffer in performance. We want to investigate using the pretrained model from the Adverse Reactions sections with no additional training and with additional training to see how it performs.",
      "factor": {
        "script": "fit_clinicalbert",
        "parameter": ["network", "epochs"],
        "labels": [
          "PreTrain:AR, Fit 0 Epochs",
          "PreTrain:BW, Fit 0 Epochs",
          "PreTrain:ALL, Fit 0 Epochs",
          "PreTrain:AR, Fit 10 Epochs",
          "PreTrain:BW, Fit 10 Epochs",
          "PreTrain:ALL, Fit 10 Epochs"
        ]
      },
      "construct_training_data": {
        "section": ["BW"]
      },
      "fit_clinicalbert": {
        "network": [
          "bestepoch-bydrug-CB_0-AR-125-all_222_24_25_1e-06_256_32.pth",
          "bestepoch-bydrug-CB_0-BW-125-all_222_24_25_1e-06_256_32.pth",
          "bestepoch-bydrug-CB_0-ALL-125-all_222_24_25_1e-06_256_32.pth"
        ],
        "epochs": [0, 10]
      }
    },

    "6": {
      "name": "Compare string before to string after",
      "description": "REPLACED BY 6A. When constructing the training data we have the choice to take words preceding the event term, following the event term, or an event split of both (default). This experiment compares the performance of those three options. This could be part of experiment 0, but that seems to be getting crowded.",
      "factor": {
        "script": "construct_training_data",
        "parameter": "method",
        "labels": [
          "After Only",
          "87.5% After",
          "75% After",
          "Before and After",
          "75% Before",
          "87.5% Before",
          "Before Only"
        ]
      },
      "construct_training_data": {
        "method": [7, 8, 9, 0, 10, 11, 6],
        "nwords": [30],
        "section": ["AR"]
      },
      "fit_clinicalbert": {
        "max-length": [32]
      }
    },

    "6A": {
      "name": "Compare string before to string after",
      "description": "Same as experiment 6, but with no max_length set. When constructing the training data we have the choice to take words preceding the event term, following the event term, or an event split of both (default). This experiment compares the performance of those three options. This could be part of experiment 0, but that seems to be getting crowded.",
      "factor": {
        "script": "construct_training_data",
        "parameter": "method",
        "labels": [
          "After Only",
          "87.5% After",
          "75% After",
          "Before and After",
          "75% Before",
          "87.5% Before",
          "Before Only"
        ]
      },
      "construct_training_data": {
        "method": [7, 8, 9, 0, 10, 11, 6],
        "nwords": [30],
        "section": ["AR"]
      }
    },

    "6B": {
      "name": "Compare string before to string after with prepend refsource",
      "description": "Same as Experiment 6A but now including refsource which was found in Experiment 0 to add value.",
      "factor": {
        "script": "construct_training_data",
        "parameter": "method",
        "labels": [
          "After Only",
          "87.5% After",
          "75% After",
          "Before and After"
        ]
      },
      "construct_training_data": {
        "method": [15, 14, 13, 12],
        "section": ["AR"]
      }
    },

    "2A": {
      "name": "Compare values for nwords, ref method 8 (87.5% after)",
      "description": "Same as experiment 2 except for reference method 8, rather than 0. The number of words we extract from the labels and use to make the examples is a core component of the reference set. Essentially the thinking is the more words the better, however, that also exponentially increases the running time. In this experiment we adjust the number of words used in the example strings from 30 up to 250 (our maximum).",
      "factor": {
        "script": "construct_training_data",
        "parameter": "nwords",
        "labels": [
          "6", "15", "30", "60", "125", "250"
        ]
      },
      "construct_training_data": {
        "method": [8],
        "nwords": [6, 15, 30, 60, 125, 250]
      },
      "fit_clinicalbert": {
        "epochs": [10]
      }
    },

    "2B": {
      "name": "Compare values for nwords, for ref method 14 (87.5% after, prepend ref source)",
      "description": "Same as experiment 2 except for reference method 8, rather than 0. The number of words we extract from the labels and use to make the examples is a core component of the reference set. Essentially the thinking is the more words the better, however, that also exponentially increases the running time. In this experiment we adjust the number of words used in the example strings from 30 up to 250 (our maximum).",
      "factor": {
        "script": "construct_training_data",
        "parameter": "nwords",
        "labels": [
          "6", "15", "30", "60", "125", "250"
        ]
      },
      "construct_training_data": {
        "method": [14],
        "nwords": [6, 15, 30, 60, 125, 250]
      },
      "fit_clinicalbert": {
        "epochs": [10]
      }
    },

    "8": {
      "name": "BW reference methods",
      "description": "Compare top three reference methods (0, 8, and 14) for Boxed Warnings. Note these are only trained on the Boxed Warnings examples. See 8A for a version where we evalaute performance of BW but trained on ALL (as was found in Experiment 4 to produce better results for the BW section).",
      "factor": {
        "script": "construct_training_data",
        "parameter": "method",
        "labels": ["0", "8", "14"]
      },
      "construct_training_data": {
        "method": [0, 8, 14],
        "section": ["BW"]
      }
    },

    "8A": {
      "name": "BW reference methods, trained on ALL",
      "description": "Compare top three reference methods (0, 8, and 14) for Boxed Warnings. This using the models trained on data from ALL sections and then the rows corresponding to Boxed Warnings are extracted. NOTE for Notebook Implementation: The extraction of the rows for BW will have to be done inline within the evaluate notebook. See Experiment 4 notebook for how to do this.",
      "factor": {
        "script": "construct_training_data",
        "parameter": "method",
        "labels": ["0", "8", "14"]
      },
      "construct_training_data": {
        "method": [0, 8, 14],
        "section": ["ALL"]
      }
    },

    "9": {
      "name": "ADR Term Identification Method",
      "description": "Compare different methods of identifying ADR terms from text, exact string match vs deepcadrme.",
      "factor": {
        "script": "fit_clinicalbert",
        "parameter": ["refsource", "network"],
        "labels": [
          "All: ClinicalBERT",
          "Exact Only: ClinicalBERT",
          "All: PubMedBERT",
          "Exact Only: PubMedBERT"
        ]
      },
      "construct_training_data": {
        "method": [8]
      },
      "fit_clinicalbert": {
        "refsource": ["all", "exact"],
        "network": ["Bio_ClinicalBERT", "microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract"]
      }
    },

    "9A": {
      "name": "ADR Term Identification Method, prepend refsource",
      "description": "Compare different methods of identifying ADR terms from text, exact string match vs deepcadrme.",
      "factor": {
        "script": "fit_clinicalbert",
        "parameter": ["refsource", "network"],
        "labels": [
          "All: ClinicalBERT",
          "Exact Only: ClinicalBERT",
          "All: PubMedBERT",
          "Exact Only: PubMedBERT"
        ]
      },
      "construct_training_data": {
        "method": [14]
      },
      "fit_clinicalbert": {
        "refsource": ["all", "exact"],
        "network": ["Bio_ClinicalBERT", "microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract"]
      }
    },

    "10": {
      "name": "TAC Training/Testing Split",
      "description": "Use the TAC training and testing splits instead of randomly splitting.",
      "factor": {
        "script": "fit_clinicalbert",
        "parameter": ["split-method", "network"],
        "labels": [
          "Default (80/10/10): CB",
          "TAC (45/5/50): CB",
          "Default (80/10/10): PMB",
          "TAC (45/5/50): PMB"
        ]
      },
      "construct_training_data": {
        "method": [8],
        "nwords": [125]
      },
      "fit_clinicalbert": {
        "split-method": ["24", "TAC"],
        "network": ["Bio_ClinicalBERT", "microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract"]
      }
    },

    "10A": {
      "name": "TAC Training/Testing Split - prepend refsource",
      "description": "Use the TAC training and testing splits instead of randomly splitting.",
      "factor": {
        "script": "fit_clinicalbert",
        "parameter": ["split-method", "network"],
        "labels": [
          "Default (80/10/10): CB",
          "TAC (45/5/50): CB",
          "Default (80/10/10): PMB",
          "TAC (45/5/50): PMB"
        ]
      },
      "construct_training_data": {
        "method": [14],
        "nwords": [125]
      },
      "fit_clinicalbert": {
        "split-method": ["24", "TAC"],
        "network": ["Bio_ClinicalBERT", "microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract"]
      }
    },

    "10B": {
      "name": "TAC Training/Testing Split - prepend refsource, nwords explore",
      "description": "Use the TAC training and testing splits instead of randomly splitting.",
      "factor": {
        "script": ["construct_training_data", "fit_clinicalbert"],
        "parameter": ["nwords", "split-method"],
        "xlabels": [
          "Nwords=60, Default Split (80/10/10)",
          "Nwords=125, Default Split (80/10/10)",
          "Nwords=60, TAC (45/5/50)",
          "Nwords=125, TAC (45/5/50)"
        ]
      },
      "construct_training_data": {
        "method": [14],
        "nwords": [60, 125]
      },
      "fit_clinicalbert": {
        "split-method": ["24", "TAC"]
      }
    }

  },

  "replicates": {
    "0": [1],
    "2": [1]
  },

  "deployments": {
    "V01-AR": {
      "name": "Onsides Version 01 - Adverse Reactions",
      "description": "V01 of the model to extract side effect terms from the Adverse Reactions section of the labels, created on April 29, 2022. These paramters were chosen through analysis of the results of Experiments 1 through 7. Max F1 (threshold = 2.397) was 0.87 on the validation set and 0.86 on the test set. AUROC was 0.88 on the validation set and 0.88 on the test set. AUPR was 0.91 on the validation set and 0.91 on the test set.",
      "construct_training_data": {
        "method": [8],
        "nwords": [125],
        "section": ["AR"]
      },
      "fit_clinicalbert": {
        "epochs": [10],
        "refsource": ["exact"]
      }
    },
    "V02-AR": {
      "name": "Onsides Version 02 - Adverse Reactions",
      "description": "V02 of the model to extract side effect terms from the Adverse Reactions section of the labels, created on October 24, 2022. These paramters were chosen through analysis of the results of Experiments 1 through 10. Max F1 (threshold = 1.848) was 0.880 on the validation set and 0.868 on the test set. AUROC was 0.922 on the validation set and 0.909 on the test set. AUPR was 0.942 on the validation set and 0.928 on the test set. When evaluated against the TAC gold standard, the model achieves: F1=0.881, AUROC=0.895, and AUPR=0.921.",
      "construct_training_data": {
        "method": [14],
        "nwords": [125],
        "section": ["AR"]
      },
      "fit_clinicalbert": {
        "epochs": [25]
      }
    },
    "V01-BW": {
      "name": "Onsides Version 01 - Boxed Warnings",
      "description": "V01 of the model to extract side effect terms from the Boxed Warnings section of the labels, created on April 29, 2022. These paramters were chosen through analysis of the results of Experiments 1 through 7. Max F1 was 0.70 on the validation set and 0.66 on the test set. AUROC was 0.74 on the validation set and 0.71 on the test set. AUPR was 0.72 on the validation set and 0.60 on the test set.",
      "construct_training_data": {
        "method": [8],
        "nwords": [125],
        "section": ["BW"]
      },
      "fit_clinicalbert": {
        "epochs": [25],
        "refsource": ["exact"]
      }
    },
    "V02-BW": {
      "name": "Onsides Version 02 - Boxed Warnings",
      "description": "V02 of the model to extract side effect terms from the Boxed Warnings section of the labels, created on April 29, 2022. These paramters were chosen through analysis of the results of Experiments 1 through 9. Max F1 was 0.683 on the validation set and 0.657 on the test set. AUROC was 0.821 on the validation set and 0.798 on the test set. AUPR was 0.660 on the validation set and 0.657 on the test set.",
      "construct_training_data": {
        "method": [8],
        "nwords": [125],
        "section": ["BW"]
      },
      "fit_clinicalbert": {
        "epochs": [25],
        "refsource": ["all"]
      }
    }

  }
}
